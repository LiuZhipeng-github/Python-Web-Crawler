•Requests库的7个主要方法
      方法                             说明
requests.requests()   构造一个请求，支撑以下各方法的基础方法
requests.get()        获取HTML网页的主要方法，对应于HTTP的GET
requests.head()       获取HTML网页头信息的方法，对应于HTTP的HEAD
requests.post()       向HTML网页提交POST请求的方法，对应于HTTP的POST
requests.put()        向HTML网页提交PUT请求的方法，对应于HTTP的PUT
requests.patch()      向HTML网页提交局部修改请求，对应于HTTP的PATCH
requests.delete()     向HTML网页提交删除请求，对应于HTTP的DELETE


•Requests库的get()方法
r = requests.get(url)
构造一个向服务器请求资源的Request对象
返回一个包含服务器资源的Response对象

requests.get(url,params=None,**kwargs)
•url:拟获取页面的url链接
•params:url中的额外参数，字典或字节流格式，可选
•**kwargs:12个控制访问的参数

Response对象的属性
r.status_code        HTTP请求的返回状态，200表示连接成功，404表示连接失败
r.text               HTTP响应内容的字符串形式，即，url对应的页面内容
r.encoding           从HTTP header中猜测的响应内容编码方式
r.apparent_encoding  从内容中分析出的响应内容编码方式(备选编码方式)    
r.content            HTTP响应内容的二进制形式
•r.encoding:如果header中不存在charset,则认为编码为ISO-8859-1
•apparent_encoding:根据网页内容分析出的编码方式,更加准确


爬取网页的通用代码框架
•理解Requests库的异常
requests.ConnectionError        网络连接错误异常，如DNS查询失败、拒绝连接等
requests.HTTPError              HTTP错误异常
requests.URLRequired            URL缺失异常
requests.TooManyRedirects       超过最大重定向次数，产生重定向异常
requests.ConnectTimeout         连接远程服务器超时异常
requests.Timeout                请求URL超时，产生超时异常

r.raise_for_status()            如果不是200，产生异常requests.HTTPError

HTTP协议
Hypertext Transfer Protocol,超文本传输协议
HTTP是一个基于"请求与相应"模式的、无状态的应用层协议
HTTP协议一般采用URL作为定位网络资源的标识
URL格式 http://host[:port][path]
host:合法的Internet主机域名或IP地址
port:端口号，缺省端口为80
path:请求资源的路径

HTTP URL理解:
URL是通过http协议存取资源的Internet路径，一个URL对应一个数据资源

HTTP协议对资源的操作
GET      请求获取URL位置的资源
HEAD     请求获取URL位置资源的响应报告，即获得该资源的头部信息
POST     请求向URL位置的资源后附加新的数据
PUT      请求向URL位置存储一个资源，覆盖原URL位置的资源
PATCH    请求局部更新URL位置的资源，即改变该处资源的部分内容
DELETE   请求删除URL位置存储的资源

理解PATCH和PUT的区别
假设URL位置有一组数据UserInfo，包括UserID、UserName等20个字段
需求：用户修改了UserName，其他不变
•采用PATCH，仅向URL提交UserName的局部更新请求(节省网络带宽)
•采用PUT，必须将所有20个字段一并提交到URL，未提交字段将被删除
HTTP协议与Requests库的功能是一一对应的



requests.request(method,url,**kwargs)
•method:请求方式，对应get/put/post等7种方法
r = requests.request('GET',url,**kwargs)
r = requests.request('HEAD',url,**kwargs)
r = requests.request('POST',url,**kwargs)
r = requests.request('PUT',url,**kwargs)
r = requests.request('PATCH',url,**kwargs)
r = requests.request('delete',url,**kwargs)
r = requests.request('OPTIONS',url,**kwargs)
•url:拟获取页面的url链接
•**kwargs:控制访问的参数，共13个

params：字典或字节序列，作为参数增加到url中
>>> kv = {'key1':'value1','key2':'value2'}
>>> r = requests.request('GET','http://python123.io/ws',params=kv)
>>>print(r.url)
http://python123.io/ws?key1=value1&key2=value2

data:字典、字节序列或文件对象，作为Request的内容
>>> kv = {'key1':'value1','key2':'value2'}
>>> r = requests.request('POST','http://python123.io/ws',data=kv)
>>> body = '主体内容'
>>> r = requests.request('POST','http://python123.io/ws',data=body)

json:JSON格式的数据，作为Request的内容
>>> kv = {'key1':'value1'}
>>> r = requests.request('POST','http://python123.io/ws',json=kv)

headers:字典,HTTP定制头
>>> hd = {'user-agent':'Chrome/10'}
>>> r = requests.request('POST','http://python123.io/ws',headers=hd)

cookies:字典或CookieJar,Request中的cookie
auth:元组，支持HTTP认证功能

files:字典类型,传输文件
>>> fs = {'file':open('data.xls','rn')}
>>> r = requests.request('POST','http://python123.io/ws',files=fs)

timeout:设定超时时间，秒为单位
>>> r = requests.request('GET','http://www.baidu.com',timeout=10)

proxies:字典类型,设定访问代理服务器，可以增加登录认证
>>> pxs = {'http':'http://user:pass@10.10.10.1:1234'
           'https':'https://10.10.10.1:4321'  }
>>> r = requests.request('http://www.baidu.com',proxies=pxs)



allow_redirects:True/False,默认为True,重定向开关
stream:True/False,默认为True,获取内容立即下载开关
verify:True/False,默认为True,认证SSL证书开关
cert:本地SSL证书路径

requests.head(url,**kwargs)
url:拟获取页面的url链接
**kwargs:13个控制访问的参数

requests.post(url,data=None,json=None,**kwargs)
url:拟更新页面的url链接
data:字典、字节序列或文件，Request的内容
json:JSON格式的数据，Request的内容
**kwargs:11个控制访问的参数

requests.put(url,data=None,**kwargs)
url:拟更新页面的url链接
data:字典、字节序列或文件，Request的内容
**kwargs:12个控制访问的参数

requests.patch(url,data=None,**kwargs)
url:拟更新页面的url链接
data:字典、字节序列或文件，Request的内容
**kwargs:12个控制访问的参数

requests.delete(url,**kwargs)
url:拟更新页面的url链接
**kwargs:13个控制访问的参数


网路爬虫的尺寸

小规模，数据量小，爬取速度不敏感    爬取网页，玩转网页        Requests库
中规模，数据规模较大，爬取速度敏感  爬取网站，爬取系列网站    Scrapy库
大规模，搜索引擎，爬取速度关键      爬取全网                 定制开发

       
网络爬虫引发的问题
•网络爬虫的骚扰
    •受限于编写水平和目的，网路爬虫将会为web服务器带来巨大的资源开销
•网络爬虫的法律风险
    •服务器上的数据有产权归属
    •网络爬虫获取数据后牟利将带来法律风险
•网络爬虫泄露隐私
    •网络爬虫可能具备突破简单访问控制的能力，获得被保护数据从而泄露个人隐私

网络爬虫的限制
•来源审查：判断User-Agent进行限制
    •检查来访HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问
•发布公告:Robots协议
    •告知所有爬虫网站dev爬取策略，要求爬虫遵守

Robots协议
Robot Exclusion Standard 网络爬虫排除标准
作用：网站告知网络爬虫哪些页面可以抓取，哪些不行
形式：在网站根目录下的robots.txt文件
基本语法：# 注释,*代表所有,/代表根目录
User-agent:*
Disallow:/

Robots协议的使用
网络爬虫：自动或人工识别robots.txt，再进行内容爬取
约束性：Robots协议是建议但非约束性的，网络爬虫可以不遵守，但存在法律风险