Scrapy爬虫框架结构
•爬虫框架
    •爬虫框架是实现爬虫功能的一个软件结构和功能组件集合
    •爬虫框架是一个半成品,能够帮助用户实现专业网络爬虫
•"5+2"结构
Engine:
    •控制所有模块之间的数据流
    •根据条件触发事件
    •不需要用户修改
Downloader:
    •根据请求下载网页
    •不需要用户修改
Scheduler:
    •对爬取请求进行调度管理
    •不需要用户修改
Downloader Middleware:
    •目的:实施Engine、Scheduler和Downloader之间进行用户可配置的控制
    •功能:修改、丢弃、新增请求或响应
Spider:
    •解析Downloader返回的响应
    •产生爬取项
    •产生额外的爬取请求
Item Pipelines
    •以流水线方式处理Spider产生的爬取项
    •由一组操作组成,类似流水线,每个操作是一个Item Pipeline类型
    •可能操作包括:清理、检验和查重爬取项中的HTML数据、将数据存储到数据库
Spider Middleware
    •目的:对请求和爬取项的再处理
    •功能:修改、丢弃、新增请求或爬取项

requests vs. Scrapy
相同点
•两者都可以进行页面请求和爬取，Python爬虫的两个重要技术路线
•两者可用性都好,文档丰富,入门简单
•两者都没有处理js、提交表单、应对验证码等功能(可拓展)
       requests			         Scrapy
页面级爬虫			网站级爬虫
功能库				框架
并发性考虑不足,性能较差		并发性好,性能较高
重点在于页面下载			重点在于爬虫框架
定制灵活				一般定制灵活、深度定制困难
上手十分容易			入门稍难

选用哪个技术路线开发爬虫
•非常小的需求,requests库
•不太小的需求,Scrapy框架
•定制程度很高的需求,requests>Scrapy

Scrapy命令行
Scrapy是为持续运行设计的专业爬虫框架,提供操作的Scrapy命令行

格式
scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  commands
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy


为什么采用命令行运行和创建爬虫?
•命令行更容易自动化,适合脚本控制
•本质上,Scrapy是给程序员用的,功能更重要

scrapy startproject python123demo

生成的工程目录
python123demo/			外层目录
    scrapy.cfg			部署Scrapy爬虫的配置文件
    python123demo/      	Scrapy框架的用户自定义Python代码
        __init__.py		初始化脚本
        items.py		Items代码模板(继承类)
        middlewares.py  	Middlewares代码模板(继承类)
        pipelines.py		Pipelines代码模板(继承类)
	settings.py		Scrapy爬虫的配置文件
	spiders/		Spiders代码模板目录(继承类)
            __init__.py		初始文件,无需修改
	    __pycache__/	缓存目录,无需修改


产生步骤
步骤1:建立一个Scrapy爬虫工程
步骤2:在工程中产生一个Scrapy爬虫
步骤3:配置产生的spider爬虫


yield关键字
•生成器是一个不断产生值的函数
•包含yield语句的函数是一个生成器
•生成器每次产生一个值(yield语句),函数被冻结,被唤醒后再产生一个值

为何要有生成器
•生成器相比一次列出所有内容的优势
    •更节省存储空间
    •响应更迅速
    •使用更灵活

Scrapy爬虫的使用步骤
步骤1:创建一个工程和Spider模板
步骤2:编写Spider
步骤3:编写Item Pipeline
步骤4:优化配置策略

爬虫的数据类型
•Request类
class scrapy.http.Request()
Request对象表示一个http请求
由Spider生成,由Downloader执行

属性或方法				说明
.url			Request对应的请求URL地址
.method			对应的请求方法,'GET''POST'等
.headers		字典类型风格的请求头
.body			请求内容主题,字符串类型
.meta			用户添加的扩展信息,在Scrapy内部模块间传递信息使用
.copy()			复制该请求



•Response类
class scrapy.http.Response()
Request对象表示一个http响应
由Downloader生成,由Spider处理

属性或方法				说明
.url			Response对应的URL地址
.status			HTTP状态码,默认200
.headers		Response对应的头部信息
.body			Response对应的内容信息,字符串类型
.flags			一组标记
.request		产生Response类型对应的Request对象
.copy()			复制该响应


•Item类
class scrapy.item.Item()
Item对象表示一个从HTML页面中提取的信息内容
由Spider生成,由Item Pipeline处理
Item类似字典类型,可以按照字典类型操作

Scrapy爬虫支持多种HTML信息提取方法
Beautiful Soup
lxml
re
XPath Selector
CSS Selector

CSS Selector的基本使用
<HTML>.css('a::attr(href)').extract()
      标签名称      标签属性


